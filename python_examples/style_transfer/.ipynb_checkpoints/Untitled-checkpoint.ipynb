{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = \"banner.jpg\"\n",
    "style_path = \"../../appli/style38.jpg\"\n",
    "\n",
    "style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite')\n",
    "style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 69: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c61f68ae1c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Load the input images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcontent_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mstyle_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c61f68ae1c71>\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path_to_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Function to load an image from a file, and add a batch dimension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    559\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m       return read_file_eager_fallback(\n\u001b[0m\u001b[0;32m    562\u001b[0m           filename, name=name, ctx=_ctx)\n\u001b[0;32m    563\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m    600\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m    601\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 69: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Function to load an image from a file, and add a batch dimension.\n",
    "def load_img(path_to_img):\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  img = img[tf.newaxis, :]\n",
    "\n",
    "  return img\n",
    "\n",
    "# Function to pre-process by resizing an central cropping it.\n",
    "def preprocess_image(image, target_dim):\n",
    "  # Resize the image so that the shorter dimension becomes 256px.\n",
    "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
    "  short_dim = min(shape)\n",
    "  scale = target_dim / short_dim\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "  image = tf.image.resize(image, new_shape)\n",
    "\n",
    "  # Central crop the image.\n",
    "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
    "\n",
    "  return image\n",
    "\n",
    "# Load the input images.\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "# Preprocess the input images.\n",
    "preprocessed_content_image = preprocess_image(content_image, 384)\n",
    "preprocessed_style_image = preprocess_image(style_image, 256)\n",
    "\n",
    "print('Style Image Shape:', preprocessed_style_image.shape)\n",
    "print('Content Image Shape:', preprocessed_content_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(preprocessed_content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(preprocessed_style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run style prediction on preprocessed style image.\n",
    "def run_style_predict(preprocessed_style_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_predict_path)\n",
    "\n",
    "  # Set model input.\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_style_image)\n",
    "\n",
    "  # Calculate style bottleneck.\n",
    "  interpreter.invoke()\n",
    "  style_bottleneck = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return style_bottleneck\n",
    "\n",
    "# Calculate style bottleneck for the preprocessed style image.\n",
    "style_bottleneck = run_style_predict(preprocessed_style_image)\n",
    "print('Style Bottleneck Shape:', style_bottleneck.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run style transform on preprocessed style image\n",
    "def run_style_transform(style_bottleneck, preprocessed_content_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
    "\n",
    "  # Set model input.\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Set model inputs.\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_content_image)\n",
    "  interpreter.set_tensor(input_details[1][\"index\"], style_bottleneck)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Transform content image.\n",
    "  stylized_image = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return stylized_image\n",
    "\n",
    "# Stylize the content image using the style bottleneck.\n",
    "stylized_image = run_style_transform(style_bottleneck, preprocessed_content_image)\n",
    "\n",
    "# Visualize the output.\n",
    "imshow(stylized_image, 'Stylized Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate style bottleneck of the content image.\n",
    "style_bottleneck_content = run_style_predict(\n",
    "    preprocess_image(content_image, 256)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content blending ratio between [0..1].\n",
    "# 0.0: 0% style extracts from content image.\n",
    "# 1.0: 100% style extracted from content image.\n",
    "f  = plt.figure()\n",
    "content_blending_ratio = 0.5 \n",
    "\n",
    "# Blend the style bottleneck of style image and content image\n",
    "style_bottleneck_blended = content_blending_ratio * style_bottleneck_content + (1 - content_blending_ratio) * style_bottleneck\n",
    "\n",
    "# Stylize the content image using the style bottleneck.\n",
    "stylized_image_blended = run_style_transform(style_bottleneck_blended,\n",
    "                                             preprocessed_content_image)\n",
    "\n",
    "# Visualize the output.\n",
    "imshow(stylized_image_blended, 'Blended Stylized Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig('filename.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
